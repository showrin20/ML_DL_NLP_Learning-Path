

# **Naive Bayes Classifier**
**Definition:** A probabilistic classifier based on Bayes' theorem, assuming conditional independence of features.

**Steps:**
1. **Training Phase:**
   - Calculate prior probability: P(y)
   - Calculate conditional probability for each feature: P(f|y)

2. **Prediction Phase:**
   - For new data, calculate: P(y|f1, f2, ..., fn)
   - Choose the class with the highest probability.

**Formula:**
\[
P(y|X) \propto P(y) \prod_{i} P(f_i|y)
\]

**Example:** For the sentence *"predictable with no fun"*:
- Calculate P(positive) and P(negative) using word counts.
- Multiply probabilities and select the higher value.

**Smoothing:** Avoids zero probabilities by adding a constant (Laplace Smoothing).
\[
P(w_i|c) = \frac{count(w_i, c) + 1}{\sum_{w \in V} count(w, c) + |V|}
\]

**Variants:**
1. **Multinomial Naive Bayes:** Based on word frequency.
2. **Binary Multinomial Naive Bayes:** Only considers word presence (not frequency).

**Strengths:** Simple, efficient, good for high-dimensional data.
**Weaknesses:** Assumes independence, sensitive to feature correlations.



### 5. **ML Evaluation**

**1. Data Splitting:**
- **Train set:** Model training.
- **Validation set:** Parameter tuning.
- **Test set:** Final evaluation.
- **Cross-validation:** K-fold method to use all data efficiently.

**2. Performance Metrics:**
- **Accuracy:** \( \frac{TP + TN}{TP + TN + FP + FN} \)
- **Precision:** \( \frac{TP}{TP + FP} \) (How many predicted positives were correct?)
- **Recall (Sensitivity):** \( \frac{TP}{TP + FN} \) (How many actual positives were detected?)
- **F1 Score:** Harmonic mean of precision and recall.
- **Confusion Matrix:** Table showing TP, FP, TN, FN.

**Example Calculation:**
|                | Predicted Positive | Predicted Negative |
|-|--|--|
| **Actual Positive** | TP = 50            | FN = 10            |
| **Actual Negative** | FP = 5             | TN = 100           |

- Precision = 50 / (50 + 5) = 0.91
- Recall = 50 / (50 + 10) = 0.83
- F1 Score = 2 × (0.91 × 0.83) / (0.91 + 0.83) = 0.87

**3. Addressing Issues:**
- **Overfitting:** Use regularization, simplify model.
- **Underfitting:** Increase model complexity, collect more data.

**4. Statistical Significance:**
- **Null Hypothesis (H0):** No difference between models.
- **T-test:** Checks if observed improvement is statistically significant.



### 6. **Applications and Use Cases**
1. **Text Classification:** Spam detection, sentiment analysis.
2. **Sentiment Analysis:** Extracting positive or negative opinions.
3. **Language Identification:** Determining language of a text.
4. **Authorship Attribution:** Identifying the author based on writing style.



### 7. **Key Takeaways**
- Understand feature representation methods (BoW, N-grams, Lexicons).
- Master conditional probability and Bayes' theorem.
- Be proficient with Naive Bayes and its variants.
- Use appropriate evaluation metrics.
- Avoid overfitting and underfitting.





# **Naive Bayes, Text Classification, and Sentiment Analysis**

## **1. Introduction to Naive Bayes and Classification**
Classification is a fundamental task in machine learning, involving assigning a category to an input. Applications include:
- Recognizing text, images, or speech
- Sorting emails (spam detection)
- Sentiment analysis (determining positive or negative sentiment)
- Language identification
- Authorship attribution
- Document categorization
- Medical diagnosis

Naive Bayes is a simple yet effective probabilistic classifier, often used for text classification due to its efficiency and interpretability.



## **2. Naive Bayes Classifier**
The Naive Bayes classifier is based on **Bayes' Theorem**:
\[
P(c|d) = \frac{P(d|c) P(c)}{P(d)}
\]
where:
- \( P(c|d) \) is the probability of class \( c \) given document \( d \)
- \( P(d|c) \) is the probability of document \( d \) occurring given class \( c \)
- \( P(c) \) is the prior probability of class \( c \)
- \( P(d) \) is the probability of document \( d \) occurring

Since \( P(d) \) is constant for all classes, we use:
\[
\hat{c} = \arg\max_{c \in C} P(d|c) P(c)
\]
This makes Naive Bayes a **generative model**, assuming a document is generated by first picking a class and then generating words based on class probability.



## **3. Assumptions of Naive Bayes**
### **Bag-of-Words Assumption**
- The order of words does not matter.
- Only word frequency is considered.
- Assumes words contribute independently to the classification.

### **Conditional Independence Assumption**
- Words are independent given the class.
- The probability of a word depends only on the class and not on other words.

Thus, we can simplify:
\[
P(d|c) = P(w_1, w_2, ..., w_n | c) = \prod_{i=1}^{n} P(w_i | c)
\]



## **4. Types of Naive Bayes Models**
Naive Bayes classifiers can be of different types depending on how they model the distribution of data:

### **Multinomial Naive Bayes**
- Assumes word frequencies follow a multinomial distribution.
- Used for text classification, where word occurrences are counted.

### **Bernoulli Naive Bayes**
- Features are binary (presence or absence of a word in a document).
- Commonly used for spam detection.

### **Gaussian Naive Bayes**
- Assumes continuous features follow a normal distribution.
- Applied in cases like medical diagnosis and fraud detection.



## **5. Training the Naive Bayes Classifier**
### **Prior Probability Calculation**
For class \( c \):
\[
P(c) = \frac{N_c}{N_{doc}}
\]
where:
- \( N_c \) is the number of documents in class \( c \)
- \( N_{doc} \) is the total number of documents

### **Likelihood Estimation**
Using **Maximum Likelihood Estimation (MLE)**:
\[
P(w_i | c) = \frac{count(w_i, c)}{\sum_{w \in V} count(w, c)}
\]
where \( V \) is the vocabulary set.

### **Laplace (Add-One) Smoothing**
To avoid zero probabilities:
\[
P(w_i | c) = \frac{count(w_i, c) + 1}{\sum_{w \in V} (count(w, c) + 1)}
\]

### **Handling Unknown Words**
- If a word in the test document is unseen in training, it should be ignored.
- Alternatively, smoothing techniques like add-k smoothing can assign small probabilities to unseen words.



## **6. Worked Example**
Given the following training data:

| Class | Document |
|-|-|
| - | just plain boring |
| - | entirely predictable and lacks energy |
| - | no surprises and very few laughs |
| + | very powerful |
| + | the most fun film of the summer |

And a test document: **"predictable with no fun"**
- Compute \( P(c) \), \( P(w|c) \)
- Apply Naive Bayes formula
- Ignore unknown words
- Final classification: **negative (-)**



## **7. Optimizing for Sentiment Analysis**
### **Binary Multinomial Naive Bayes**
Instead of word frequency, we consider only presence/absence of words.

### **Handling Negation**
A simple method:
- Add **"NOT_"** prefix to words after a negation word (e.g., "not", "never").
- Example:
  - "I didn’t like this movie" → "I didn’t NOT_like NOT_this NOT_movie"

### **Using Sentiment Lexicons**
Instead of all words, use predefined positive and negative word lists, such as:
- MPQA Subjectivity Lexicon
- Opinion Lexicon
- General Inquirer



## **8. Avoiding Harms in Classification**
### **Bias and Representation Harms**
- Example: Some sentiment classifiers give lower sentiment scores for African-American names.
- **Toxicity Detection Risks**: Misclassifying discussions about minority groups as offensive.
- **Solution**: Use fairness-aware datasets, ensure demographic balance in training data.

### **Model Cards**
- Documentation describing a model’s training data, evaluation, and biases.
- Helps users understand model limitations.



## **9. Summary**
- Naive Bayes is a simple, efficient text classifier.
- It assumes conditional independence and a bag-of-words model.
- It is widely used in spam detection, sentiment analysis, and language ID.
- Evaluation involves precision, recall, and F1-score.
- Cross-validation ensures robust results.
- Bias and fairness concerns must be addressed.



