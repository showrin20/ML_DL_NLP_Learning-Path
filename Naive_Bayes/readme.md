

# **Naive Bayes Classifier**
**Definition:** A probabilistic classifier based on Bayes' theorem, assuming conditional independence of features.

**Steps:**
1. **Training Phase:**
   - Calculate prior probability: P(y)
   - Calculate conditional probability for each feature: P(f|y)

2. **Prediction Phase:**
   - For new data, calculate: P(y|f1, f2, ..., fn)
   - Choose the class with the highest probability.

**Formula:**
\[
P(y|X) \propto P(y) \prod_{i} P(f_i|y)
\]

**Example:** For the sentence *"predictable with no fun"*:
- Calculate P(positive) and P(negative) using word counts.
- Multiply probabilities and select the higher value.

**Smoothing:** Avoids zero probabilities by adding a constant (Laplace Smoothing).
\[
P(w_i|c) = \frac{count(w_i, c) + 1}{\sum_{w \in V} count(w, c) + |V|}
\]

**Variants:**
1. **Multinomial Naive Bayes:** Based on word frequency.
2. **Binary Multinomial Naive Bayes:** Only considers word presence (not frequency).

**Strengths:** Simple, efficient, good for high-dimensional data.
**Weaknesses:** Assumes independence, sensitive to feature correlations.



### 5. **ML Evaluation**

**1. Data Splitting:**
- **Train set:** Model training.
- **Validation set:** Parameter tuning.
- **Test set:** Final evaluation.
- **Cross-validation:** K-fold method to use all data efficiently.

**2. Performance Metrics:**
- **Accuracy:** \( \frac{TP + TN}{TP + TN + FP + FN} \)
- **Precision:** \( \frac{TP}{TP + FP} \) (How many predicted positives were correct?)
- **Recall (Sensitivity):** \( \frac{TP}{TP + FN} \) (How many actual positives were detected?)
- **F1 Score:** Harmonic mean of precision and recall.
- **Confusion Matrix:** Table showing TP, FP, TN, FN.

**Example Calculation:**
|                | Predicted Positive | Predicted Negative |
|-|--|--|
| **Actual Positive** | TP = 50            | FN = 10            |
| **Actual Negative** | FP = 5             | TN = 100           |

- Precision = 50 / (50 + 5) = 0.91
- Recall = 50 / (50 + 10) = 0.83
- F1 Score = 2 × (0.91 × 0.83) / (0.91 + 0.83) = 0.87

**3. Addressing Issues:**
- **Overfitting:** Use regularization, simplify model.
- **Underfitting:** Increase model complexity, collect more data.

**4. Statistical Significance:**
- **Null Hypothesis (H0):** No difference between models.
- **T-test:** Checks if observed improvement is statistically significant.



### 6. **Applications and Use Cases**
1. **Text Classification:** Spam detection, sentiment analysis.
2. **Sentiment Analysis:** Extracting positive or negative opinions.
3. **Language Identification:** Determining language of a text.
4. **Authorship Attribution:** Identifying the author based on writing style.



### 7. **Key Takeaways**
- Understand feature representation methods (BoW, N-grams, Lexicons).
- Master conditional probability and Bayes' theorem.
- Be proficient with Naive Bayes and its variants.
- Use appropriate evaluation metrics.
- Avoid overfitting and underfitting.





# **Naive Bayes, Text Classification, and Sentiment Analysis**

## **1. Introduction to Naive Bayes and Classification**
Classification is a fundamental task in machine learning, involving assigning a category to an input. Applications include:
- Recognizing text, images, or speech
- Sorting emails (spam detection)
- Sentiment analysis (determining positive or negative sentiment)
- Language identification
- Authorship attribution
- Document categorization
- Medical diagnosis

Naive Bayes is a simple yet effective probabilistic classifier, often used for text classification due to its efficiency and interpretability.



## **2. Naive Bayes Classifier**
The Naive Bayes classifier is based on **Bayes' Theorem**:
\[
P(c|d) = \frac{P(d|c) P(c)}{P(d)}
\]
where:
- \( P(c|d) \) is the probability of class \( c \) given document \( d \)
- \( P(d|c) \) is the probability of document \( d \) occurring given class \( c \)
- \( P(c) \) is the prior probability of class \( c \)
- \( P(d) \) is the probability of document \( d \) occurring

Since \( P(d) \) is constant for all classes, we use:
\[
\hat{c} = \arg\max_{c \in C} P(d|c) P(c)
\]
This makes Naive Bayes a **generative model**, assuming a document is generated by first picking a class and then generating words based on class probability.



## **3. Assumptions of Naive Bayes**
### **Bag-of-Words Assumption**
- The order of words does not matter.
- Only word frequency is considered.
- Assumes words contribute independently to the classification.

### **Conditional Independence Assumption**
- Words are independent given the class.
- The probability of a word depends only on the class and not on other words.

Thus, we can simplify:
\[
P(d|c) = P(w_1, w_2, ..., w_n | c) = \prod_{i=1}^{n} P(w_i | c)
\]



## **4. Types of Naive Bayes Models**
Naive Bayes classifiers can be of different types depending on how they model the distribution of data:

### **Multinomial Naive Bayes**
- Assumes word frequencies follow a multinomial distribution.
- Used for text classification, where word occurrences are counted.

### **Bernoulli Naive Bayes**
- Features are binary (presence or absence of a word in a document).
- Commonly used for spam detection.

### **Gaussian Naive Bayes**
- Assumes continuous features follow a normal distribution.
- Applied in cases like medical diagnosis and fraud detection.



The document covers training a Naive Bayes classifier in detail. Here’s a summary of the training process:

### **Training the Naive Bayes Classifier**
1. **Class Prior Estimation**:  
   - Compute \( P(c) \), the prior probability of each class \( c \), using:
     \[
     P(c) = \frac{N_c}{N_{\text{doc}}}
     \]
     where \( N_c \) is the number of documents in class \( c \), and \( N_{\text{doc}} \) is the total number of documents.

2. **Word Likelihood Estimation**:  
   - Compute \( P(w_i | c) \), the likelihood of word \( w_i \) given class \( c \), using:
     \[
     P(w_i | c) = \frac{\text{count}(w_i, c)}{\sum_{w \in V} \text{count}(w, c)}
     \]
     where \( \text{count}(w_i, c) \) is the number of times \( w_i \) appears in all documents of class \( c \), and \( V \) is the vocabulary.

3. **Handling Zero Probabilities (Smoothing)**:  
   - Apply **Laplace smoothing (add-one smoothing)** to avoid zero probabilities:
     \[
     P(w_i | c) = \frac{\text{count}(w_i, c) + 1}{\sum_{w \in V} (\text{count}(w, c) + 1)}
     \]

4. **Building the Model**:  
   - Store \( \log P(c) \) and \( \log P(w | c) \) to prevent numerical underflow.
   - Use a **bag-of-words** assumption (word order is ignored).

### **Testing the Naive Bayes Classifier**
1. Compute the posterior probability for each class:
   \[
   c_{NB} = \arg\max_{c \in C} \log P(c) + \sum_{i \in \text{positions}} \log P(w_i | c)
   \]
2. Assign the document to the class with the highest probability.




### **Optimizations for Naive Bayes Classifier**  

To improve the performance of the Naive Bayes classifier, especially in **text classification and sentiment analysis**, several optimizations are applied:  


### **1. Binary Multinomial Naive Bayes**  
- Instead of counting word frequencies, **binary Naive Bayes** considers only whether a word appears (1) or does not appear (0) in a document.  
- This is useful in sentiment analysis, where the presence of words is more informative than their frequency.  
- **Implementation:**  
  - Convert word occurrences to binary (presence/absence).  
  - Modify the likelihood calculation to count the number of documents containing a word instead of the total word count.  
  - Helps reduce overfitting and improves generalization.  

**Example:**  
| Word | Standard Naive Bayes Count | Binary Naive Bayes Count |  
|------|-------------------------|----------------------|  
| great  | 3 | 2 |  
| boring | 2 | 1 |  
| fun  | 2 | 2 |  

---

### **2. Handling Negation in Sentiment Analysis**  
- Negation can reverse sentiment (e.g., *"not happy"* vs. *"happy"*).  
- A **simple rule-based approach** is used:
  - Append a prefix `NOT_` to words following negation words (`not, never, no, n't`) **until the next punctuation**.  

**Example:**  
- `"I didn't like this movie, but I"` → `"I didn't NOT_like NOT_this NOT_movie, but I"`  
- Words like **"NOT_like"** will be more common in negative reviews, improving classification accuracy.

---

### **3. Stop Word Removal (Optional)**  
- Common words (`the, a, is`) might not contribute much to classification.  
- **Two approaches:**
  - **Predefined stop word lists** (e.g., NLTK stop words).  
  - **Frequency-based filtering** (removing the top 10-100 most frequent words).  
- In **most text classification tasks, removing stop words doesn't improve performance** significantly, so it's often skipped.

---

### **4. Using Sentiment Lexicons**  
- When training data is **small or imbalanced**, predefined **sentiment lexicons** help improve classification.  
- **Popular lexicons:**
  - **MPQA Subjectivity Lexicon** (Wilson et al., 2005)
  - **Liu’s Opinion Lexicon** (Hu and Liu, 2004)
  - **LIWC (Linguistic Inquiry and Word Count)**
- **How it's used:**
  - Instead of individual words, add **two binary features**:  
    - `"word is in positive lexicon"`  
    - `"word is in negative lexicon"`  
  - Helps generalize better when training data is limited.

**Example:**  
| Word | In Positive Lexicon? | In Negative Lexicon? |  
|------|----------------------|----------------------|  
| love  | ✅ (1) | ❌ (0) |  
| terrible | ❌ (0) | ✅ (1) |  
| movie  | ❌ (0) | ❌ (0) |  

---

### **5. Feature Selection for Faster Training**  
- **Remove uninformative words** that don’t contribute to classification.  
- **Common feature selection methods:**
  - **Information Gain** (how much a word helps distinguish between classes).
  - **Chi-Square Test** (measures independence of word & class).
  - **Mutual Information** (quantifies dependency between word & class).  

**Example (Top 5 most informative words in sentiment analysis):**  
| Word | Information Gain |  
|------|----------------|  
| amazing  | 0.89 |  
| horrible | 0.85 |  
| waste  | 0.80 |  
| excellent | 0.75 |  
| awful | 0.72 |  

---

### **6. Laplace Smoothing (Add-One Smoothing)**
- Prevents **zero probabilities** when a word appears in test data but not in training.  
- Formula:  
  \[
  P(w_i | c) = \frac{\text{count}(w_i, c) + 1}{\sum_{w \in V} (\text{count}(w, c) + 1)}
  \]
- Ensures **every word has a small nonzero probability**.

---

### **7. Log Probability Computation (Avoiding Underflow)**
- Multiplying many small probabilities leads to **numerical underflow**.  
- Instead of:
  \[
  P(c) \times P(w_1 | c) \times P(w_2 | c) \times P(w_3 | c) ...
  \]
  We take the **logarithm** to transform multiplication into addition:
  \[
  \log P(c) + \sum_{i} \log P(w_i | c)
  \]
- This improves **stability and speed**.

---

### **Final Optimized Naive Bayes Algorithm**  
1. **Preprocess Data:**
   - Convert text to lowercase.
   - Apply **binary Naive Bayes** (optional).
   - Handle **negation** (if needed).
   - Remove **stop words** (if beneficial).
   - Use **sentiment lexicons** (if training data is small).
2. **Train Model:**
   - Compute **class priors** \( P(c) \).
   - Compute **word likelihoods** \( P(w | c) \) with **Laplace smoothing**.
   - Store **log probabilities**.
3. **Classify New Text:**
   - Compute the **log posterior** probability for each class.
   - Assign the class with the **highest probability**.
















## **6. Worked Example**
Given the following training data:

| Class | Document |
|-|-|
| - | just plain boring |
| - | entirely predictable and lacks energy |
| - | no surprises and very few laughs |
| + | very powerful |
| + | the most fun film of the summer |

And a test document: **"predictable with no fun"**
- Compute \( P(c) \), \( P(w|c) \)
- Apply Naive Bayes formula
- Ignore unknown words
- Final classification: **negative (-)**



## **7. Optimizing for Sentiment Analysis**
### **Binary Multinomial Naive Bayes**
Instead of word frequency, we consider only presence/absence of words.

### **Handling Negation**
A simple method:
- Add **"NOT_"** prefix to words after a negation word (e.g., "not", "never").
- Example:
  - "I didn’t like this movie" → "I didn’t NOT_like NOT_this NOT_movie"

### **Using Sentiment Lexicons**
Instead of all words, use predefined positive and negative word lists, such as:
- MPQA Subjectivity Lexicon
- Opinion Lexicon
- General Inquirer



## **8. Avoiding Harms in Classification**
### **Bias and Representation Harms**
- Example: Some sentiment classifiers give lower sentiment scores for African-American names.
- **Toxicity Detection Risks**: Misclassifying discussions about minority groups as offensive.
- **Solution**: Use fairness-aware datasets, ensure demographic balance in training data.

### **Model Cards**
- Documentation describing a model’s training data, evaluation, and biases.
- Helps users understand model limitations.



## **9. Summary**
- Naive Bayes is a simple, efficient text classifier.
- It assumes conditional independence and a bag-of-words model.
- It is widely used in spam detection, sentiment analysis, and language ID.
- Evaluation involves precision, recall, and F1-score.
- Cross-validation ensures robust results.
- Bias and fairness concerns must be addressed.



